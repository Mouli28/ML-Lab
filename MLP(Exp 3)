{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNPW02NsXeMdCn7ZPAaCpcz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"shl5oh7ewGJM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710836703626,"user_tz":-330,"elapsed":4,"user":{"displayName":"Mouliraj A K","userId":"09516782249211112966"}},"outputId":"539b20b1-fd66-40ec-b23d-7be57f9d437f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1:\n","Error: [[0.57536833]]\n","Updated Weights Input-Hidden:\n","[[ 0.29156557  0.10442216]\n"," [-0.20843443  0.40442216]\n"," [ 0.2        -0.3       ]\n"," [ 0.09156557  0.40442216]]\n","Updated Weights Hidden-Output:\n","[[-0.24377049]\n"," [ 0.28221413]]\n","Updated Bias Hidden:\n","[-0.20401227  0.09598773]\n","Updated Bias Output:\n","[-0.18754098]\n","\n","Epoch 2:\n","Error: [[0.52518176]]\n","Updated Weights Input-Hidden:\n","[[ 0.28518199  0.11021059]\n"," [-0.21481801  0.41021059]\n"," [ 0.2        -0.3       ]\n"," [ 0.08518199  0.41021059]]\n","Updated Weights Hidden-Output:\n","[[-0.19215332]\n"," [ 0.3589973 ]]\n","Updated Bias Hidden:\n","[-0.20460742  0.09539258]\n","Updated Bias Output:\n","[-0.08277106]\n","\n"]}],"source":["import numpy as np\n","\n","class MLP:\n","    def __init__(self, input_size, hidden_size, output_size, weights_input_hidden, weights_hidden_output, learning_rate=0.01):\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.output_size = output_size\n","        self.learning_rate = learning_rate\n","\n","        self.weights_input_hidden = weights_input_hidden\n","        self.weights_hidden_output = weights_hidden_output\n","\n","        self.bias_hidden = np.array([-0.2, 0.1])\n","        self.bias_output = np.array([-0.3])\n","\n","    def sigmoid(self, x):\n","        return 1 / (1 + np.exp(-x))\n","\n","    def sigmoid_derivative(self, x):\n","        return x * (1 - x)\n","\n","    def forward_propagation(self, inputs):\n","\n","        hidden_input = np.dot(inputs, self.weights_input_hidden) + self.bias_hidden\n","        hidden_output = self.sigmoid(hidden_input)\n","        output_input = np.dot(hidden_output, self.weights_hidden_output) + self.bias_output\n","        output = self.sigmoid(output_input)\n","\n","        return hidden_output, output, hidden_input, output_input\n","\n","    def backward_propagation(self, inputs, targets, hidden_output, output, hidden_input, output_input):\n","\n","        error = targets - output\n","        output_delta = error * self.sigmoid_derivative(output)\n","        hidden_error = np.dot(output_delta, self.weights_hidden_output.T)\n","        hidden_delta = hidden_error * self.sigmoid_derivative(hidden_output)\n","        self.weights_hidden_output += np.dot(hidden_output.T, output_delta) * self.learning_rate\n","        self.weights_input_hidden += np.dot(inputs.T, hidden_delta) * self.learning_rate\n","        self.bias_output += np.sum(output_delta) * self.learning_rate\n","        self.bias_hidden += np.sum(hidden_delta) * self.learning_rate\n","\n","        return error\n","\n","input_size = 4\n","hidden_size = 2\n","output_size = 1\n","weights_input_hidden = np.array([[0.3, 0.1],\n","                                 [-0.2, 0.4],\n","                                 [0.2, -0.3],\n","                                 [0.1, 0.4]])\n","weights_hidden_output = np.array([[-0.3], [0.2]])\n","\n","mlp = MLP(input_size=input_size, hidden_size=hidden_size, output_size=output_size,\n","          weights_input_hidden=weights_input_hidden, weights_hidden_output=weights_hidden_output,\n","          learning_rate=0.8)\n","\n","X_train = np.array([[1, 1, 0, 1]])\n","y_train = np.array([[1]])\n","\n","for epoch in range(2):\n","    print(f\"Epoch {epoch+1}:\")\n","    hidden_output, output, hidden_input, output_input = mlp.forward_propagation(X_train)\n","    error = mlp.backward_propagation(X_train, y_train, hidden_output, output, hidden_input, output_input)\n","    print(\"Error:\", error)\n","    print(\"Updated Weights Input-Hidden:\")\n","    print(mlp.weights_input_hidden)\n","    print(\"Updated Weights Hidden-Output:\")\n","    print(mlp.weights_hidden_output)\n","    print(\"Updated Bias Hidden:\")\n","    print(mlp.bias_hidden)\n","    print(\"Updated Bias Output:\")\n","    print(mlp.bias_output)\n","    print()"]},{"cell_type":"code","source":["import numpy as np\n","\n","class Perceptron:\n","    def __init__(self, num_features, learning_rate=0.01):\n","        self.weights = np.zeros(num_features)\n","        self.bias = 0\n","        self.learning_rate = learning_rate\n","\n","    def predict(self, inputs):\n","        print(inputs)\n","        print(\"______________________\")\n","        weighted_sum = np.dot(inputs, self.weights) + self.bias\n","        activation = self.activate(weighted_sum)\n","        return 1 if activation > 0 else 0\n","\n","    def train(self, inputs, target):\n","        prediction = self.predict(inputs)\n","        error = target - prediction\n","        self.weights += self.learning_rate * error * inputs\n","        self.bias += self.learning_rate * error\n","        print(error, self.weights, self.bias)\n","        print(\"------------------------------\")\n","\n","    def activate(self, x):\n","        return 1 / (1 + np.exp(-x))\n","\n","if __name__ == \"__main__\":\n","    X_train = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n","    y_train = np.array([0, 0, 0, 1])\n","    perceptron = Perceptron(num_features=2)\n","    for _ in range(100):\n","        for inputs, target in zip(X_train, y_train):\n","            perceptron.train(inputs, target)\n","\n","    test_inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n","    for inputs in test_inputs:\n","        prediction = perceptron.predict(inputs)\n","        print(f\"Input: {inputs}, Prediction: {prediction}\")"],"metadata":{"id":"2hp0dMnR2bwA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"JMaYa5z0-xsx"},"execution_count":null,"outputs":[]}]}